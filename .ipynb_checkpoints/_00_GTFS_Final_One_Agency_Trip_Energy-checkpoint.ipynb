{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7523b19a-f5cb-44af-8f14-15e61b692f08",
   "metadata": {},
   "source": [
    "## Authors: Zhaocai Liu, Phoebe Ho\n",
    "## Purpose: This notebook is used to process GTFS data for the RouteE-BEAT Tool (Get Energy Consumption Estimation for Each Trip based on Mapmatching and RouteE Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b34e3-bc9b-4231-8d2f-b628a0ca4636",
   "metadata": {},
   "source": [
    "# Create dataframe with trips and corresponding shapes (get GPS traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebce98-60ac-4df7-badc-a0cc54131b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from geopy.distance import great_circle\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091ab85-d854-4981-8cae-d7ea4f076fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select city for analysis\n",
    "city = 'saltlake'\n",
    "\n",
    "# load data\n",
    "df_shape = pd.read_csv(f'./GTFS_Data/{city}/shapes.txt', sep=',', header=0)\n",
    "df_route = pd.read_csv(f'./GTFS_Data/{city}/routes.txt', sep=',', header=0)\n",
    "df_trips = pd.read_csv(f'./GTFS_Data/{city}/trips.txt', sep=',', header=0)\n",
    "df_stops = pd.read_csv(f'./GTFS_Data/{city}/stops.txt', sep=',', header=0)\n",
    "df_stops_times = pd.read_csv(f'./GTFS_Data/{city}/stop_times.txt', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d898a-6615-4ebe-86bb-70de65ff7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_route = pd.merge(df_trips, df_route, how='left', on='route_id')\n",
    "trip_route = trip_route[trip_route['route_type'] == 3]  # select only bus services (route_type = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2ad6b-6356-443c-a4d7-92761afe4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only consider bus shapes\n",
    "df_shape = df_shape[df_shape.shape_id.isin(trip_route.shape_id.unique())]\n",
    "df_shape = df_shape.sort_values(by = ['shape_id','shape_pt_sequence'])\n",
    "df_shape_list = [group for _,group in df_shape.groupby('shape_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1292f-c723-4f46-8bfb-a3408bb57002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####-------------\n",
    "## Define a function to upsample shape GPS to 1s trace and get timestamp for each point\n",
    "\n",
    "import datetime\n",
    "def upsample_shape(df_tmp):\n",
    "    \"\"\"\n",
    "    This function is used to upsample a shape file to 1s (8m)\n",
    "    input: df_tmp for one individual shape\n",
    "    output: upsampled shape \n",
    "    \"\"\"\n",
    "    # Shift latitude and longitude to get previous point\n",
    "    df_tmp['prev_latitude'] = df_tmp['shape_pt_lat'].shift()\n",
    "    df_tmp['prev_longitude'] = df_tmp['shape_pt_lon'].shift()\n",
    "    \n",
    "    # Calculate the distance between consecutive points using great_circle\n",
    "    df_tmp['distance_km'] = df_tmp.apply(lambda row: great_circle(\n",
    "        (row['prev_latitude'], row['prev_longitude']),  # Previous point\n",
    "        (row['shape_pt_lat'], row['shape_pt_lon'])  # Current point\n",
    "    ).kilometers if pd.notnull(row['prev_latitude']) else 0, axis=1)\n",
    "    \n",
    "    # Calculate total distance\n",
    "    total_distance_km = df_tmp['distance_km'].sum()\n",
    "\n",
    "    ## Define a random date \n",
    "    date_tmp = datetime.datetime(2023, 9, 3)  \n",
    "    ## Speed is assumed to be 30 km/h, which is about 10 (8.33) m per second/node\n",
    "    df_tmp['segment_duration_delta'] = df_tmp['shape_dist_traveled']/df_tmp['shape_dist_traveled'].max() * datetime.timedelta(seconds=round(total_distance_km / 30 * 3600))\n",
    "    df_tmp['segment_duration_delta'] =  df_tmp['segment_duration_delta'].apply(lambda x: datetime.timedelta(seconds=round(x.total_seconds())))\n",
    "    df_tmp['timestamp'] = datetime.timedelta(seconds=0) + df_tmp['segment_duration_delta'] + date_tmp\n",
    "    \n",
    "    ## Upsample to 1s \n",
    "    shape_id_tmp = df_tmp.shape_id.iloc[0]\n",
    "    df_tmp = df_tmp[['shape_pt_lat','shape_pt_lon','timestamp','shape_dist_traveled']].drop_duplicates(subset = ['timestamp']).set_index('timestamp').resample('1s').interpolate(method='linear')\n",
    "    \n",
    "    ## Now we have the 1 HZ gps trace for each trip with timestamp\n",
    "    \n",
    "    df_tmp = df_tmp.reset_index(drop = True)\n",
    "    \n",
    "    df_tmp['shape_id'] = shape_id_tmp\n",
    "\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5171ab-5b39-469d-bda8-c627d434341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize before and after upsampling\n",
    "df_test = df_shape_list[10]\n",
    "\n",
    "gdf_shape = gpd.GeoDataFrame(df_test, geometry=gpd.points_from_xy(df_test.shape_pt_lon, df_test.shape_pt_lat))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 10))\n",
    "gdf_shape.plot(ax = ax1,markersize = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0d914-5553-4d40-a0d7-3b49419fbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_upsample = upsample_shape(df_test)\n",
    "gdf_shape_upsample = gpd.GeoDataFrame(df_test_upsample, geometry=gpd.points_from_xy(df_test_upsample.shape_pt_lon, df_test_upsample.shape_pt_lat))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 10))\n",
    "gdf_shape_upsample.plot(ax = ax1,markersize = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4e58f-118e-403b-8385-a483673b0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now mapmatching \n",
    "import sqlalchemy as sql\n",
    "from mappymatch.constructs.coordinate import Coordinate\n",
    "from mappymatch.constructs.geofence import Geofence\n",
    "from mappymatch.constructs.trace import Trace\n",
    "from mappymatch.matchers.lcss.lcss import LCSSMatcher\n",
    "from nrel.mappymatch.readers.tomtom import read_tomtom_nxmap_from_sql\n",
    "from nrel.mappymatch.readers.tomtom_config import TomTomConfig \n",
    "\n",
    "### Engine for mapmatching\n",
    "user = \"zliu2\"\n",
    "password = \"NRELisgr8!\"\n",
    "engine = sql.create_engine(\n",
    "    f\"postgresql://{user}:{password}@trolley.nrel.gov:5432/master\"\n",
    ")\n",
    "\n",
    "### MapMatching should be conducted for each individual shapes to avoid duplicate mapmatching for each trip\n",
    "def get_matches(df_tmp):\n",
    "    df_tmp = df_tmp.drop_duplicates(\n",
    "      subset = ['shape_pt_lat', 'shape_pt_lon'],\n",
    "      keep = 'first').reset_index(drop = True)\n",
    "    trace = Trace.from_dataframe(df_tmp, lat_column ='shape_pt_lat', lon_column='shape_pt_lon')\n",
    "    geofence = Geofence.from_trace(trace, padding=1e3)\n",
    "    config = TomTomConfig(include_display_class=True,include_direction = True)\n",
    "    nxmap = read_tomtom_nxmap_from_sql(engine, geofence, tomtom_config=config)\n",
    "    matcher = LCSSMatcher(nxmap)\n",
    "    matches = matcher.match_trace(trace).matches_to_dataframe()\n",
    "    df_result = pd.concat([df_tmp, matches], axis=1)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a8d16-d42e-4a5d-9f54-617e691c46ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Conduct upsampling and mapmatching using multiprocessing\n",
    "\n",
    "# use multiprocessing to process all the files\n",
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(2)\n",
    "output = pool.map(upsample_shape, df_shape_list)\n",
    "pool.close()\n",
    "\n",
    "df_match_list = []\n",
    "for df_tmp in output:\n",
    "    df_match_list.append(get_matches(df_tmp))\n",
    "output_df = pd.concat(df_match_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f95cb-3713-44c9-aec3-be0f9d235c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a single column\n",
    "output_df.rename(columns={'trip_id': 'shape_id'}, inplace=True)\n",
    "output_df.to_csv('mapmatched_shape.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7fef9a-56f5-43c9-b532-d3818d69600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2dff8-4190-4169-b15e-b1558a106bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_shape_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c80249-d968-4eac-b618-c1627d57cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test mapmatching\n",
    "get_matches(df_test_upsample).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ea404-49b3-4519-a185-8fad54a4f1f4",
   "metadata": {},
   "source": [
    "# Now mergeg trip with shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb873942-ae6d-4c61-a107-3f2244418621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "## Read in mapmatched shape\n",
    "output_df = pd.read_csv('mapmatched_shape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6d702-ad90-49fc-b012-2011fa4b57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2749bb8-3d5f-4ad6-beec-bc731043cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops_times_group = df_stops_times.groupby('trip_id').agg({'arrival_time':'first', 'departure_time':'last',\\\n",
    "                                                               'stop_id':['first','last']}).reset_index()\n",
    "\n",
    "df_stops_times_group.columns = ['trip_id', 'o_time','d_time','o_stop_id','d_stop_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e887cc-c212-4b06-ae7c-a5b2ec39f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge trip_route with df_stops_times_group\n",
    "\n",
    "trip_route = pd.merge(trip_route, df_stops_times_group, how='left', on='trip_id')\n",
    "trip_route['o_time'] = pd.to_timedelta(trip_route['o_time'])\n",
    "trip_route['d_time'] = pd.to_timedelta(trip_route['d_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18772ce7-950c-4191-9413-ccd7b51f94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_route['trip_duration'] = trip_route['d_time'] - trip_route['o_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7fc8ba-be79-422c-b92c-3fde6283233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with shapes to get GPS traces for each trip\n",
    "trip_shape = pd.merge(trip_route[['trip_id','shape_id','o_time','d_time']], output_df, how='left', on='shape_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849685a8-57e7-4026-9ce1-3d9705787439",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_shape = trip_shape.sort_values(by=['trip_id', 'shape_dist_traveled']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba02e0-6794-4ecf-a395-4425b7e9cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate approximate timestamps for each GPS trace\n",
    "list_trip_shape = [item for _,item in trip_shape.groupby('trip_id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ddfc5-8d15-46ed-8719-e057825db575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####-------------\n",
    "## Define a function to upsample shape GPS to 1s trace and get timestamp for each point\n",
    "\n",
    "import datetime\n",
    "def attach_timestamp(df_tmp):\n",
    "    ## Calculate the travel time\n",
    "    df_tmp['segment_duration_delta'] = df_tmp['shape_dist_traveled']/df_tmp['shape_dist_traveled'].max() * (df_tmp['d_time'] - df_tmp['o_time'])\n",
    "    df_tmp['segment_duration_delta'] =  df_tmp['segment_duration_delta'].apply(lambda x: datetime.timedelta(seconds=round(x.total_seconds())))\n",
    "    df_tmp['timestamp'] = df_tmp['o_time'] + df_tmp['segment_duration_delta']\n",
    "    \n",
    "    ## get hour and minute of gps timestamp\n",
    "    df_tmp[\"Datetime_nearest5\"] = df_tmp['timestamp'].dt.round(\"5min\")\n",
    "    df_tmp['hour'] = df_tmp['Datetime_nearest5'].dt.components['hours']  \n",
    "    df_tmp['minute'] = df_tmp['Datetime_nearest5'].dt.components['minutes'] \n",
    "\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3d6d9-b4e6-4e6a-9ac1-a4f7eebe763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use multiprocessing to process all the files\n",
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "output_trip = pool.map(attach_timestamp, list_trip_shape)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cf6a8-2d54-4e2e-a5e7-c733f604a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check one result\n",
    "output_trip[200].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f41c8-7ee0-4439-8163-be14773b280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test adding timestamp \n",
    "\n",
    "df_timestamp = attach_timestamp(list_trip_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb76d4a-9099-4f42-9919-c445e4246045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae06f6-bb05-4f62-9193-61a5a3860fe7",
   "metadata": {},
   "source": [
    "# Now we do gradeit and attaching time-dependent speed/time information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e44ba-bdeb-4ae9-8fb6-eeef626f614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "from gradeit.gradeit import gradeit\n",
    "from shapely.geometry import Polygon, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e3c31-6c2e-4f07-934f-ee97b85a484c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre-process the speed profiles\n",
    "profile_dir = '/projects/mbap/data/amazon-eco/us_network/profile_id_mapping.csv'\n",
    "profile_ids = pd.read_csv(profile_dir)\n",
    "profile_ids['datetime'] = pd.to_datetime(profile_ids['time_slot'], unit='s')\n",
    "profile_ids['hour'] = profile_ids.datetime.dt.hour\n",
    "profile_ids['minute'] = profile_ids.datetime.dt.minute\n",
    "profile_ids['profile_id'] = profile_ids['profile_id'].astype('str')  # Convert from UUID\n",
    "\n",
    "\n",
    "### Show an example data\n",
    "profile_ids[(profile_ids['profile_id']=='15836046-d4a5-4f03-8710-eda07cf98f60') & \\\n",
    "                (profile_ids['hour']==19)]\n",
    "# profile_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08706534-3785-43c6-9174-184e8084a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######----------------------\n",
    "## For each GPS trace, get the corresponding link speed, grade, and traffic signal info\n",
    "# Define functions\n",
    "\n",
    "def add_speed_profile(df_match):\n",
    "    # get profile ids for road links\n",
    "    road_key_list = df_match.road_key.unique().tolist()\n",
    "    speed_profile_query = f\"\"\"\n",
    "    select * from network_w_speed_profiles\n",
    "    where netw_id in {tuple(road_key_list)}  \n",
    "    \"\"\"\n",
    "    speed_profiles = pd.read_sql(speed_profile_query, con=engine)\n",
    "    speed_profiles['link_direction'] = speed_profiles['link_direction'].astype('int64')\n",
    "    speed_profiles['netw_id'] = speed_profiles['netw_id'].astype('str')\n",
    "    df_merge = pd.merge(df_match, speed_profiles, how='left', \n",
    "                        left_on=['road_key','direction'], \n",
    "                        right_on=['netw_id','link_direction'])\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def get_stacked_profiles(df):\n",
    "    stacked_profiles = pd.melt(df[['index','monday_profile_id', 'tuesday_profile_id','wednesday_profile_id',\n",
    "                    'thursday_profile_id', 'friday_profile_id',\n",
    "                    'saturday_profile_id', 'sunday_profile_id',]],\n",
    "    id_vars=['index']).sort_values('index').reset_index(drop=True)\n",
    "    stacked_profiles.columns = ['index','day_of_week','profile_id']\n",
    "    stacked_profiles['day_of_week'] = stacked_profiles['day_of_week'].str.extract('(.+)_profile_id', expand=True)\n",
    "    return stacked_profiles\n",
    "\n",
    "\n",
    "def extract_traffic_signals(junction_id_list):\n",
    "    # get traffic signals for specified road links\n",
    "    signal_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM tomtom_multinet_current.MNR_Traffic_Light nt2tl\n",
    "    WHERE nt2tl.JUNCTION_ID IN {tuple(junction_id_list)}\n",
    "    \"\"\"\n",
    "    signals = pd.read_sql(signal_query, con=engine)\n",
    "    signals_agg = signals.groupby(['junction_id'])['feat_id'].count().reset_index()\n",
    "    signals_agg = signals_agg.rename(columns={\"feat_id\": \"traffic_signal\",})\n",
    "    \n",
    "    # join signal count to main df\n",
    "    signals_agg['junction_id'] = signals_agg['junction_id'].astype('str')  # Convert from UUID   \n",
    "    return signals_agg\n",
    "\n",
    "def add_signal_features_to_dataframe(df):\n",
    "    junction_id_list = pd.concat([df.junction_id_from, df.junction_id_to], \n",
    "                                 ignore_index=True).dropna()\n",
    "    junction_id_list = junction_id_list.unique().tolist()\n",
    "    signals = extract_traffic_signals(junction_id_list)\n",
    "    signals.junction_id = signals.junction_id.astype('str')\n",
    "    \n",
    "    # merge with main dataframe \n",
    "    df.junction_id_from = df.junction_id_from.astype('str')\n",
    "    df.junction_id_to = df.junction_id_to.astype('str')\n",
    "    df = pd.merge(df, signals, how='left', left_on='junction_id_from', right_on='junction_id')\n",
    "    df = pd.merge(df, signals, how='left', left_on='junction_id_to', right_on='junction_id')\n",
    "    df = df.rename(columns={\"traffic_signal_x\": \"traffic_signal_from\",\n",
    "                                          \"traffic_signal_y\": \"traffic_signal_to\",})\n",
    "    df[['traffic_signal_from', 'traffic_signal_to']] = df[['traffic_signal_from', 'traffic_signal_to']].fillna(value=0)  # fill NA with 0 (0 signals on link)\n",
    "    df = df.drop(columns=['junction_id_x','junction_id_y'])\n",
    "    df['signal_count'] = df.traffic_signal_from + df.traffic_signal_to\n",
    "    df['signal_binary'] = np.where((df['signal_count'] > 0), 1, 0)\n",
    "    return df\n",
    "\n",
    "def get_matches(gtfs_unique_pts):\n",
    "    trace = Trace.from_dataframe(gtfs_unique_pts, lat_column ='shape_pt_lat', lon_column='shape_pt_lon')\n",
    "    geofence = Geofence.from_trace(trace, padding=1e3)\n",
    "    config = TomTomConfig(include_display_class=True,include_direction = True)\n",
    "    nxmap = read_tomtom_nxmap_from_sql(engine, geofence, tomtom_config=config)\n",
    "    matcher = LCSSMatcher(nxmap)\n",
    "    matches = matcher.match_trace(trace).matches_to_dataframe()\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9bc17e-f0e3-45d0-8d99-badf47f3f1e6",
   "metadata": {},
   "source": [
    "# Add features to GTFS GPS points\n",
    "\n",
    "Grade (grade_new), speed (new_speed_mph or new_speed_kph) from the TomTom historical speed profiles, and traffic signal info (signal_binary = binary variable for if there is a traffic light at either junction of the link). Average calculated speed (gpsspeed_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d014ba0-deda-47f9-a4da-39430b237a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"zliu2\"\n",
    "password = \"NRELisgr8!\"\n",
    "\n",
    "engine = sql.create_engine(\n",
    "    f\"postgresql://{user}:{password}@trolley.nrel.gov:5432/master\"\n",
    ")\n",
    "\n",
    "raster_path = '/kfs2/projects/mbap/Zhaocai/Conda_Pack/gradeit/scripts/usgs_tiles' #'/projects/mbap/data/NED_13'\n",
    "\n",
    "KM_TO_METERS = 1000\n",
    "FT_TO_METERS = 0.3048\n",
    "FT_TO_MILES = 0.000189394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289147c6-0075-48a2-86ad-647e824ac200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gtfs(df):\n",
    "\n",
    "    # add grade\n",
    "    df_result = gradeit(df=df, lat_col='shape_pt_lat', lon_col='shape_pt_lon',\n",
    "                           filtering=True, source='usgs-local',usgs_db_path=raster_path)\n",
    "    \n",
    "    # join speed and grade information\n",
    "    df_result['miles_new'] = df_result['distances_ft'] * FT_TO_MILES  # column necessary for RouteE .predict() method 'distance_ft_filtered'\n",
    "    df_result['gpsspeed_new'] = df_result['kilometers'] / df_result['minutes']*60*0.621371  # column necessary for RouteE .predict() method\n",
    "    df_result['grade_new'] = df_result['grade_dec_filtered']  # GRADE\n",
    "    \n",
    "    df_result = add_speed_profile(df_result)\n",
    "    points_no_profile = df_result[df_result['netw_id'].isnull()].shape[0]\n",
    "    #print(f'no speed profile available for {points_no_profile} GPS points (out of {len(df_result)})')\n",
    "    \n",
    "    df_result['index'] = df_result.index.values\n",
    "    stacked_profiles = get_stacked_profiles(df_result) # create a df with index and the day of week profiles stacked (cols to rows)\n",
    "    df_result = pd.merge(df_result, stacked_profiles, how='right', on='index')\n",
    "    \n",
    "    # get relative speed % by profile id and hour\n",
    "    df_result['profile_id'] = df_result['profile_id'].astype('str')  # Convert from UUID    \n",
    "    df_result = pd.merge(df_result, profile_ids, how='left',\n",
    "                        on=['profile_id','hour','minute'])\n",
    "    \n",
    "    # calculate speed for timestamp\n",
    "    df_result['new_speed_kph'] = df_result['free_flow_speed'] * (df_result['relative_speed']/1000)\n",
    "    df_result['new_speed_mph'] = df_result['new_speed_kph'] * 0.6213711922  # kph to mph \n",
    "    df_result = add_signal_features_to_dataframe(df_result)\n",
    "    df_result = df_result[['trip_id', 'o_time', 'd_time',\n",
    "           'shape_pt_lat', 'shape_pt_lon',\n",
    "           'shape_dist_traveled', 'timestamp',\n",
    "           'road_key', 'link_direction', 'direction',\n",
    "           'miles_new', 'gpsspeed_new', 'grade_new',\n",
    "           'free_flow_speed', \n",
    "           'day_of_week','new_speed_kph', 'new_speed_mph',\n",
    "           'traffic_signal_from', 'traffic_signal_to', 'signal_count',\n",
    "           'signal_binary']]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ab795-6bd1-451f-ad06-2fd73fa15c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test process_gtfs function\n",
    "df_timestamp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a98a3d-9569-4739-bf0f-1b2765674308",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test process_gtfs function\n",
    "process_gtfs(df_timestamp).dropna(subset = ['new_speed_mph']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647c880-409d-428d-a194-1f45f096ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check nan values for speed\n",
    "print(process_gtfs(df_timestamp).shape[0])\n",
    "print(process_gtfs(df_timestamp).dropna(subset = ['new_speed_mph']).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5996d24-55c3-4ea1-937c-c71b4dd265e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seems like there are NAN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49071d24-3871-44ec-9f28-103e848beb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_gtfs(df_timestamp).shape[0]/df_timestamp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399009b-8357-48dc-bbbc-283e1c35a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b2acf-6cab-4f61-83c8-2721298c68af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1bfdb-2c40-4b9c-ad99-c00db9407d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use multiprocessing to process all the files\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(10)\n",
    "output = pool.map(process_gtfs, output_trip[0:1000])\n",
    "pool.close()\n",
    "output_df = pd.concat(output)\n",
    "output_df.to_csv(f'gtfs_features_{city}_1.csv', index=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aee25f-ac64-4158-a961-789ffb531624",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdf789-7303-49f6-b530-072846230a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
